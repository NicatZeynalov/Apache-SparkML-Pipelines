{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic operations using Apache SparkML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook Iâ€™ll use the HMP dataset and perform some basic operations using Apache SparkML Pipeline component. This dataset is a public collection of labelled accelerometer data recordings to be used for the creation and validation of acceleration models of human motion primitives. Let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20200305085727-0000\n",
      "KERNEL_ID = bd96fc56-1d60-4444-ab91-399120bcba83\n",
      "Cloning into 'HMP_Dataset'...\n",
      "remote: Enumerating objects: 865, done.\u001b[K\n",
      "remote: Total 865 (delta 0), reused 0 (delta 0), pack-reused 865\u001b[K\n",
      "Receiving objects: 100% (865/865), 1010.96 KiB | 0 bytes/s, done.\n",
      "Checking out files: 100% (848/848), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/wchill/HMP_Dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brush_teeth\tDrink_glass  Liedown_bed  Sitdown_chair  final.py\r\n",
      "Climb_stairs\tEat_meat     MANUAL.txt   Standup_chair  impdata.py\r\n",
      "Comb_hair\tEat_soup     Pour_water   Use_telephone\r\n",
      "Descend_stairs\tGetup_bed    README.txt   Walk\r\n"
     ]
    }
   ],
   "source": [
    "!ls HMP_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt\r\n",
      "Accelerometer-2011-04-11-13-29-54-brush_teeth-f1.txt\r\n",
      "Accelerometer-2011-05-30-08-35-11-brush_teeth-f1.txt\r\n",
      "Accelerometer-2011-05-30-09-36-50-brush_teeth-f1.txt\r\n",
      "Accelerometer-2011-05-30-10-34-16-brush_teeth-m1.txt\r\n",
      "Accelerometer-2011-05-30-21-10-57-brush_teeth-f1.txt\r\n",
      "Accelerometer-2011-05-30-21-55-04-brush_teeth-m2.txt\r\n",
      "Accelerometer-2011-05-31-15-16-47-brush_teeth-f1.txt\r\n",
      "Accelerometer-2011-06-02-10-42-22-brush_teeth-f1.txt\r\n",
      "Accelerometer-2011-06-02-10-45-50-brush_teeth-f1.txt\r\n",
      "Accelerometer-2011-06-06-10-45-27-brush_teeth-f1.txt\r\n",
      "Accelerometer-2011-06-06-10-48-05-brush_teeth-f1.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls HMP_Dataset/Brush_teeth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "schema = StructType([StructField(\"x\", IntegerType(), True), StructField(\"y\", IntegerType(), True), StructField(\"z\", IntegerType(), True)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Data\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file = os.listdir('HMP_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_filtered = [s for s in file if '_' in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Brush_teeth',\n",
       " 'Climb_stairs',\n",
       " 'Comb_hair',\n",
       " 'Descend_stairs',\n",
       " 'Drink_glass',\n",
       " 'Eat_meat',\n",
       " 'Eat_soup',\n",
       " 'Getup_bed',\n",
       " 'Liedown_bed',\n",
       " 'Pour_water',\n",
       " 'Sitdown_chair',\n",
       " 'Standup_chair',\n",
       " 'Use_telephone']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df = None\n",
    "for category in file_filtered:\n",
    "    data_files = os.listdir('HMP_Dataset/'+category)\n",
    "    for data_file in data_files:\n",
    "        temp_df = spark.read.option(\"header\",\"false\").option(\"delimiter\", \" \").csv(\"HMP_Dataset/\"+category+\"/\"+data_file, schema = schema)\n",
    "        \n",
    "        temp_df = temp_df.withColumn('class', lit(category))\n",
    "        temp_df = temp_df.withColumn('source', lit(data_file))\n",
    "        \n",
    "        if df is None:\n",
    "            df =  temp_df\n",
    "        else:\n",
    "            df = df.union(temp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+\n",
      "|  x|  y|  z|      class|              source|\n",
      "+---+---+---+-----------+--------------------+\n",
      "| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 21| 52| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 51| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 20| 50| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 52| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 50| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 22| 51| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 21| 51| 33|Brush_teeth|Accelerometer-201...|\n",
      "| 20| 50| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n",
      "| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n",
      "| 20| 51| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 18| 49| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 19| 48| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 16| 53| 34|Brush_teeth|Accelerometer-201...|\n",
      "| 18| 52| 35|Brush_teeth|Accelerometer-201...|\n",
      "| 18| 51| 32|Brush_teeth|Accelerometer-201...|\n",
      "+---+---+---+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline : Index the Class String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+----------+\n",
      "|  x|  y|  z|      class|              source|classIndex|\n",
      "+---+---+---+-----------+--------------------+----------+\n",
      "| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|\n",
      "+---+---+---+-----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "Indexer = StringIndexer(inputCol ='class', outputCol = 'classIndex')\n",
    "indexed = Indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. One-hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCol = 'classIndex', outputCol = 'Categoryvec')\n",
    "encoded = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+----------+--------------+\n",
      "|  x|  y|  z|      class|              source|classIndex|   Categoryvec|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+\n",
      "| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Transform x, y, z to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssambler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"], outputCol ='features')\n",
    "\n",
    "featuresVectorized = vectorAssambler.transform(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+\n",
      "|  x|  y|  z|      class|              source|classIndex|   Categoryvec|        features|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+\n",
      "| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|\n",
      "| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|\n",
      "| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|\n",
      "| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|\n",
      "| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,52.0,34.0]|\n",
      "| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,34.0]|\n",
      "| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,35.0]|\n",
      "| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,34.0]|\n",
      "| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,50.0,34.0]|\n",
      "| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,35.0]|\n",
      "| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,51.0,33.0]|\n",
      "| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,34.0]|\n",
      "| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|\n",
      "| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|\n",
      "| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,51.0,35.0]|\n",
      "| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,49.0,34.0]|\n",
      "| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[19.0,48.0,34.0]|\n",
      "| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[16.0,53.0,34.0]|\n",
      "| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,52.0,35.0]|\n",
      "| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,51.0,32.0]|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featuresVectorized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. p-norm Normalization\n",
    "1-norm: Taxicab Norm or Manhattan Norm\n",
    "1-norm is simply the sum of the absolute values of the columns.\n",
    "\n",
    "$$||x||_1 := \\Sigma^n_{i=1}|x_i|$$\n",
    "\n",
    "2-norm: Euclidian Norm\n",
    "\n",
    "$$||x||_2 := \\sqrt{x_1^2 + ... + x_n^2}$$\n",
    "p-norm\n",
    "\n",
    "$$||X||_p := (\\Sigma^n_{i=1} |X_i|^p)^{1/p}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
      "|  x|  y|  z|      class|              source|classIndex|   Categoryvec|        features|       features_norm|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
      "| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
      "| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
      "| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
      "| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
      "| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n",
      "| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n",
      "| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n",
      "| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n",
      "| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n",
      "| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n",
      "| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,51.0,33.0]|[0.2,0.4857142857...|\n",
      "| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,34.0]|[0.19230769230769...|\n",
      "| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n",
      "| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n",
      "| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,51.0,35.0]|[0.18867924528301...|\n",
      "| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,49.0,34.0]|[0.17821782178217...|\n",
      "| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[19.0,48.0,34.0]|[0.18811881188118...|\n",
      "| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[16.0,53.0,34.0]|[0.15533980582524...|\n",
      "| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,52.0,35.0]|[0.17142857142857...|\n",
      "| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,51.0,32.0]|[0.17821782178217...|\n",
      "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "normalized = Normalizer(inputCol = \"features\", outputCol = 'features_norm', p=1.0)\n",
    "\n",
    "normalized_data = normalized.transform(featuresVectorized)\n",
    "normalized_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages =[Indexer, encoder, vectorAssambler, normalized])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ibm/spark/python/pyspark/ml/wrapper.py\", line 105, in __del__\n",
      "    SparkContext._active_spark_context._gateway.detach(self._java_obj)\n",
      "AttributeError: 'Normalizer' object has no attribute '_java_obj'\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = prediction.drop('x').drop('y').drop('z').drop('class').drop('source').drop('features').drop('classIndex')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|   Categoryvec|       features_norm|\n",
      "+--------------+--------------------+\n",
      "|(12,[5],[1.0])|[0.20754716981132...|\n",
      "|(12,[5],[1.0])|[0.20754716981132...|\n",
      "|(12,[5],[1.0])|[0.20183486238532...|\n",
      "|(12,[5],[1.0])|[0.20183486238532...|\n",
      "|(12,[5],[1.0])|[0.19626168224299...|\n",
      "|(12,[5],[1.0])|[0.20560747663551...|\n",
      "|(12,[5],[1.0])|[0.19047619047619...|\n",
      "|(12,[5],[1.0])|[0.20370370370370...|\n",
      "|(12,[5],[1.0])|[0.20754716981132...|\n",
      "|(12,[5],[1.0])|[0.20370370370370...|\n",
      "|(12,[5],[1.0])|[0.2,0.4857142857...|\n",
      "|(12,[5],[1.0])|[0.19230769230769...|\n",
      "|(12,[5],[1.0])|[0.20388349514563...|\n",
      "|(12,[5],[1.0])|[0.20388349514563...|\n",
      "|(12,[5],[1.0])|[0.18867924528301...|\n",
      "|(12,[5],[1.0])|[0.17821782178217...|\n",
      "|(12,[5],[1.0])|[0.18811881188118...|\n",
      "|(12,[5],[1.0])|[0.15533980582524...|\n",
      "|(12,[5],[1.0])|[0.17142857142857...|\n",
      "|(12,[5],[1.0])|[0.17821782178217...|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results\n",
    "\n",
    "\n",
    "That's basically it. So, we have here category vector which is the target and our normalized input features are so in Spark meta vector location. In the next notebook, you will see what we can actually do with this data, but that's exactly what we wanted to achieve, and that's basically how Apache Spark and pipeline work.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
